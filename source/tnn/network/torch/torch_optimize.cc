// Tencent is pleased to support the open source community by making TNN available.
//
// Copyright (C) 2020 THL A29 Limited, a Tencent company. All rights reserved.
//
// Licensed under the BSD 3-Clause License (the "License"); you may not use this file except
// in compliance with the License. You may obtain a copy of the License at
//
// https://opensource.org/licenses/BSD-3-Clause
//
// Unless required by applicable law or agreed to in writing, software distributed
// under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR
// CONDITIONS OF ANY KIND, either express or implied. See the License for the
// specific language governing permissions and limitations under the License.

#include "torch_optimize.h"

#include <torch/csrc/jit/passes/constant_pooling.h>
#include <torch/csrc/jit/passes/dead_code_elimination.h>
#include <torch/csrc/jit/passes/fold_conv_bn.h>
#include <torch/csrc/jit/passes/remove_dropout.h>
#include <torch/csrc/jit/passes/remove_inplace_ops.h>
#include <torch/csrc/jit/passes/remove_mutation.h>
#include <torch/csrc/jit/passes/freeze_module.h>
#include <torch/csrc/jit/passes/lower_tuples.h>
#include <torch/csrc/jit/passes/subgraph_rewrite.h>

namespace torch {
namespace jit {
    int GetMaxBlockSize(Block* block) {
        int max_block_size = 0;
        for (auto it = block->nodes().begin(); it != block->nodes().end();) {
            auto* node = *it;
            it++;
            for (Block* sub_block : node->blocks()) {
                max_block_size = std::max(GetMaxBlockSize(sub_block) + 1, max_block_size);
            }
        }

        return max_block_size;
    }



    // Remove aten::list from graph
    void RemoveList(Block* block) {
        auto check_aten_list_tensor = [](torch::jit::Node* node) -> bool {
            if (node->kind() != at::aten::list) {
                return false;
            }

            at::ListTypePtr list_type = node->output(0)->type()->cast<at::ListType>();
            if (list_type && list_type->getElementType()->kind()==at::TypeKind::TensorType) {
                return true;
            }
            return false;
        };

        std::vector<Node*> deleted_nodes;

        for (auto it = block->nodes().begin(); it != block->nodes().end(); it++) {
            Node* node = *it;
            for (auto sub_block : node->blocks()) {
                RemoveList(sub_block);
            }
            if (!check_aten_list_tensor(node)) {
                continue;
            }
            node->output(0)->replaceAllUsesWith(node->input(0));
            it->removeInput(0);
            deleted_nodes.push_back(node);
        }

        for (auto del_node : deleted_nodes) {
            del_node->destroy();
        }
    }


    // Prerequisite: RemoveList() must be called first. OP aten::list must not exist in GRAPH.
    // Remove all Tensor[], aka TensorLists in the graph
    // 1. Add prim::ListUnpack after aten::split (with split_sections) if not exist.
    // 2. Store name of Tensors generated by prim::ListUnpack.
    // 3. Remove aten::_set_item, aten::__getitem__ OPs on Tensor Lists because TorchConverter cannot work on such kind of Tensor[].
    // 4. Reconstruct aten::cat() affected by aten::_set_item.
    void UnpackTensorLists(Graph* graph, Block* block) {
        auto check_listunpack = [](torch::jit::Node* node) -> bool {
            if (node->kind() != c10::prim::ListUnpack) {
                return false;
            }
            return true;
        };

        auto check_aten_split_sections_without_listunpack = [](torch::jit::Node* node) -> bool {
            if (node->kind() != at::aten::split) {
                return false;
            }
            at::ListTypePtr list_type = node->output(0)->type()->cast<at::ListType>();
            if (list_type && toIValue(node->input(1)).value().isList()) {
                auto split_users = node->output(0)->uses();
                for (int i=0; i<split_users.size(); i++) {
                    if (split_users[i].user->kind()==c10::prim::ListUnpack) {
                        return false;
                    }
                }
            }
            return true;
        };

        auto check_aten_setitem = [](torch::jit::Node* node) -> bool {
            if (node->kind() != at::aten::_set_item) {
                return false;
            }
            if (at::ListTypePtr list_type = node->input(0)->type()->cast<at::ListType>()) {
                if (list_type->getElementType()->kind()==at::TypeKind::TensorType) {
                    return true;
                }
            }
            return false;
        };

        auto check_aten_getitem = [](torch::jit::Node* node) -> bool {
            if (node->kind() != at::aten::__getitem__) {
                return false;
            }
            if (at::ListTypePtr list_type = node->input(0)->type()->cast<at::ListType>()) {
                if (list_type->getElementType()->kind()==at::TypeKind::TensorType) {
                    return true;
                }
            }
            return false;
        };

        auto check_aten_cat_setted_list = [](torch::jit::Node* node, const std::set<std::string>& setted_tensor_lists) -> bool {
            if (node->kind() != at::aten::cat) {
                return false;
            }
            if (setted_tensor_lists.find(node->input(0)->debugName()) != setted_tensor_lists.end()) {
                return true;
            }
            return false;
        };

        std::vector<Node*> deleted_nodes;
        std::set<std::string> setted_tensor_lists;
        std::map<std::string, std::vector<Value*>> tensor_list_nodemap;

        for (auto it = block->nodes().begin(); it != block->nodes().end(); it++) {
            Node* node = *it;
            for (auto sub_block : node->blocks()) {
                UnpackTensorLists(graph, sub_block);
            }

            if (!check_listunpack(node) && !check_aten_split_sections_without_listunpack(node) &&
                !check_aten_setitem(node) && !check_aten_getitem(node) &&
                !check_aten_cat_setted_list(node, setted_tensor_lists)) {
                continue;
            }

            if (check_listunpack(node)) {
                // In most cases, there is no need to record existing prim::ListUnpack
                // however, if prim::ListUnpack already exists after aten::split,
                // but aten::_set_item and aten::__getitem__ still use such aten::split ans input.
                // BUG will occur if we do not store info about the existing prim::ListUnpack.
                const int outputs_size = node->outputs().size();
                std::vector<Value*> nodemap;
                for (int i=0; i<outputs_size; i++) {
                    nodemap.push_back(node->output(i));
                }
                tensor_list_nodemap[node->input(0)->debugName()] = nodemap;
            } else if (check_aten_split_sections_without_listunpack(node)){
                int num_split_outputs = TNN_NS::conversion::getValue<std::vector<int64_t>>(node->input(1)).size();
                Node* new_list_unpack_node = graph->createListUnpack(node->output(0), num_split_outputs);
                std::vector<Value*> nodemap;
                for (int i=0; i<num_split_outputs; i++) {
                    nodemap.push_back(new_list_unpack_node->output(i));
                }
                tensor_list_nodemap[node->output(0)->debugName()] = nodemap;
                new_list_unpack_node->insertAfter(node);
            } else if (check_aten_setitem(node)) {
                std::string list_name = node->input(0)->debugName();
                int set_index = static_cast<int>(TNN_NS::conversion::getValue<int64_t>(node->input(1)));
                tensor_list_nodemap[list_name][set_index] = node->input(2);
                deleted_nodes.push_back(node);
                setted_tensor_lists.insert(list_name);
            } else if (check_aten_getitem(node)) {
                std::string list_name = node->input(0)->debugName();
                int get_index = static_cast<int>(TNN_NS::conversion::getValue<int64_t>(node->input(1)));
                Value* target_value = tensor_list_nodemap[list_name][get_index];
                node->output(0)->replaceAllUsesWith(target_value);
                deleted_nodes.push_back(node);
            } else if (check_aten_cat_setted_list(node, setted_tensor_lists)) {
                Node* new_list_construct_node = graph->createList(c10::TensorType::get(), tensor_list_nodemap[node->input(0)->debugName()]);
                new_list_construct_node->insertBefore(node);
                node->replaceInput(0, new_list_construct_node->output(0));
            }
        }

        for (auto del_node : deleted_nodes) {
            std::string out0_name = del_node->output(0)->debugName();
            del_node->destroy();
        }
    }


    void RemoveListAppend(Graph* graph, Block* block) {
        auto check_node = [](torch::jit::Node* node) -> bool {
            if (!(node->kind() == aten::append && node->inputs().at(0)->node()->kind() == prim::ListConstruct)) {
                return false;
            }
            if (node->owningBlock() != node->inputs().at(0)->node()->owningBlock()) {
                return false;
            }

            return true;
        };

        int max_block_size = GetMaxBlockSize(block);

        if (max_block_size > 1) {
            return;
        }

        for (auto it = block->nodes().begin(); it != block->nodes().end();) {
            auto* node = *it;
            it++;

            for (Block* sub_block : node->blocks()) {
                RemoveListAppend(graph, sub_block);
            }

            if (!check_node(node)) {
                continue;
            }

            Value* mutated_value = node->inputs().at(0);
            Node* list_node      = mutated_value->node();
            Node* new_list_node  = graph->create(prim::ListConstruct, 1);
            for (Value* input : list_node->inputs()) {
                new_list_node->addInput(input);
            }
            new_list_node->addInput(node->inputs().at(1));
            new_list_node->copyMetadata(list_node);
            new_list_node->insertAfter(node);
            new_list_node->output()->setType(list_node->output()->type());
            mutated_value->replaceAllUsesAfterNodeWith(node, new_list_node->output());
            node->destroy();
        }
    }

    void RemoveConcat(Block* block) {
        auto check_node = [](torch::jit::Node* node) -> bool {
            if (node->kind() != at::aten::cat) {
                return false;
            }
            if (node->inputs()[0]->node()->inputs().size() != 1) {
                return false;
            }

            return true;
        };

        std::vector<Node*> deleted_nodes;

        for (auto it = block->nodes().rbegin(); it != block->nodes().rend(); it++) {
            Node* node = *it;
            for (auto sub_block : node->blocks()) {
                RemoveConcat(sub_block);
            }

            if (!check_node(node)) {
                continue;
            }

            Value* input_value  = node->inputs()[0]->node()->inputs()[0];
            Value* output_value = node->outputs()[0];
            output_value->replaceAllUsesWith(input_value);
            deleted_nodes.push_back(node);
        }

        for (auto del_node : deleted_nodes) {
            del_node->destroy();
        }
    }

    void RemoveNoneTypeFromTuple(Block* block) {
        for (auto it = block->nodes().rbegin(); it != block->nodes().rend(); it++) {
            std::vector<size_t> deleted_inputs_index;
            Node* node = *it;
            for (auto sub_block : node->blocks()) {
                RemoveNoneTypeFromTuple(sub_block);
            }
            if (node->kind() != at::prim::TupleConstruct) {
                continue;
            }
            const int inputs_size = node->inputs().size();
            for (int i = 0; i < inputs_size; i++) {
                if (node->inputs()[i]->type()->kind() == c10::TypeKind::NoneType) {
                    deleted_inputs_index.push_back(i);
                }
            }
            for (const auto& index : deleted_inputs_index) {
                node->removeInput(index);
            }
        }
    }

    void RemoveException(torch::jit::Block* block) {
        auto check_node = [](torch::jit::Node* n) -> bool {
            if (n->blocks().size() != 2) {
                return false;
            }
            auto block0 = n->blocks()[0];
            auto block1 = n->blocks()[1];
            if (block0->outputs().size() != 0 || block1->outputs().size() != 0) {
                // Make sure that the node doesn't actually produce any Value that are
                // used by other nodes
                return false;
            }

            auto block0_start = block0->nodes().begin();
            auto block1_start = block1->nodes().begin();

            // Make sure that there is at least one empty block
            if (block0_start->kind() != prim::Return && block1_start->kind() != prim::Return) {
                return false;
            }

            if ((*block1_start)->kind() == prim::Return) {
                if ((*block0_start)->kind() == prim::RaiseException) {
                    if ((*(++block0_start))->kind() == prim::Return) {
                        // Make sure that block0 is solely just the exception and the return
                        return true;
                    }
                } else if ((*block0_start)->kind() == aten::format &&
                           (*(++block0_start))->kind() == prim::RaiseException) {
                    if ((*(++block0_start))->kind() == prim::Return) {
                        // Make sure that block0 is solely just the exception and the return
                        return true;
                    }
                }
            }

            if ((*block0_start)->kind() == prim::Return) {
                if ((*block1_start)->kind() == prim::RaiseException) {
                    if ((*(++block1_start))->kind() == prim::Return) {
                        // Make sure that block0 is solely just the exception and the return
                        return true;
                    }
                } else if ((*block1_start)->kind() == aten::format &&
                           (*(++block1_start))->kind() == prim::RaiseException) {
                    if ((*(++block1_start))->kind() == prim::Return) {
                        // Make sure that block0 is solely just the exception and the return
                        return true;
                    }
                }
            }

            return false;
        };

        for (auto it = block->nodes().begin(), end = block->nodes().end(); it != end; ++it) {
            for (auto b : it->blocks()) {
                RemoveException(b);
            }

            if (it->kind() == prim::If && check_node(*it)) {
                it.destroyCurrent();
            }
        }
    }

    void RemoveSlice(Block* block) {
        std::vector<Node*> deleted_nodes;

        for (auto it = block->nodes().rbegin(); it != block->nodes().rend(); it++) {
            Node* node = *it;
            for (auto sub_block : node->blocks()) {
                RemoveSlice(sub_block);
            }

            if (node->kind() != at::aten::slice) {
                continue;
            }

            const auto& inputs = node->inputs();
            const auto dim     = TNN_NS::conversion::getValue<int64_t>(inputs[1]);
            const auto start   = TNN_NS::conversion::getValue<int64_t>(inputs[2]);
            const auto end     = TNN_NS::conversion::getValue<int64_t>(inputs[3]);
            const auto step    = TNN_NS::conversion::getValue<int64_t>(inputs[4]);
            if (dim != 0 || start != 0 || step != 1 || end != LONG_LONG_MAX) {
                continue;
            }

            Value* input_value  = node->inputs()[0];
            Value* output_value = node->outputs()[0];
            output_value->replaceAllUsesWith(input_value);
            deleted_nodes.push_back(node);
        }

        for (auto del_node : deleted_nodes) {
            del_node->destroy();
        }
    }

    void RemoveClone(Block* block) {
        std::vector<Node*> deleted_nodes;

        for (auto it = block->nodes().rbegin(); it != block->nodes().rend(); it++) {
            Node* node = *it;
            for (auto block : node->blocks()) {
                RemoveClone(block);
            }
            if ((node->kind() == c10::Symbol::fromQualString("aten::clone"))) {
                Value* input_value = node->inputs()[0];
                Value* output_value = node->outputs()[0];
                output_value->replaceAllUsesWith(input_value);
                deleted_nodes.push_back(node);
            }
        }
        for (auto del_node : deleted_nodes) {
            del_node->destroy();
        }
    }

    void RemoveContiguous(std::shared_ptr<Graph> graph) {
        std::string contiguous_pattern    = R"IR(
        graph(%input, %1):
            %2 = aten::contiguous(%input, %1)
            return (%2))IR";
        std::string no_contiguous_pattern = R"IR(
        graph(%input, %1):
            return (%input))IR";

        // remove contiguous
        torch::jit::SubgraphRewriter remove_contiguous;
        remove_contiguous.RegisterRewritePattern(contiguous_pattern, no_contiguous_pattern);
        remove_contiguous.runOnGraph(graph);
    }

    // DEBUG, add output to Graph
    void DebugAddOutput(Graph* graph) {
        int curr_idx = 0;
        auto block = graph->block();
        
        std::vector<Value*> out_tuple_inputs;
        for (auto it = block->nodes().begin(); it != block->nodes().end(); it++) {
            Node* node = *it;
            if (node->kind()==c10::prim::TupleConstruct) {
                std::cout << "=== [Graph DEBUG] c10::prim::TupleConstruct, ";
                std::cout << ", in0.name=" << node->input(0)->debugName();
                std::cout << ", out0.name=" << node->output(0)->debugName() << " ==="<< std::endl;
                for (int i=0; i<node->inputs().size(); i++) {
                    out_tuple_inputs.push_back(node->input(i));
                }
            }
        }
        // by name and type
        std::string target_out_name = "attn_weights3.2";
        for (auto it = block->nodes().begin(); it != block->nodes().end(); it++) {
            Node* node = *it;
            if (node->inputs().size()>0 && node->outputs().size()>0 &&
                node->output(0)->debugName()==target_out_name) {
                
                out_tuple_inputs[0] = node->output(0);
                Node* new_tuple_construct_node = graph->createTuple(out_tuple_inputs);
                graph->registerOutput(new_tuple_construct_node->output(0));
                graph->insertNode(new_tuple_construct_node);
                graph->eraseOutput(0);
                std::cout << "=== [Graph DEBUG] register out0 of [" << curr_idx << "]th node to output. ";
                std::cout << "Target node type = " << node->kind().toQualString() << ", in0.name=" << node->input(0)->debugName();
                std::cout << ", out0.name=" << node->output(0)->debugName() << " ==="<< std::endl;
                break;
            }
            curr_idx++;
        }
    }

    void TorchOptPass(script::Module& module) {

        module.eval();
        module = torch::jit::freeze_module(module);
        auto graph = module.get_method("forward").graph();
        LowerSimpleTuples(graph);

        removeDropout(module);
        RemoveException(graph->block());
        RemoveListAppend(graph.get(), graph->block());
//      Remove Concat cause cascade rcnn crash        
//        RemoveConcat(graph->block());
        RemoveContiguous(graph);
        
        RemoveList(graph->block());
        UnpackTensorLists(graph.get(), graph->block()); // Call to RemoveList() Required Before Call to UnpackTensorList()!
//        RemoveClone(graph->block());
//        RemoveNoneTypeFromTuple(graph->block());
//        RemoveSlice(graph->block());

        torch::jit::EliminateDeadCode(graph);
        // DEBUG
        //DebugAddOutput(graph.get());
    }
}  // namespace jit
}  // namespace torch
